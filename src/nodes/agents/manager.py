from ast import operator
from typing import Annotated, Literal, TypedDict
from langchain.agents import AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferMemory
from langchain.agents import create_tool_calling_agent
from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import BaseModel, Field
import yaml
import uuid
from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

from src.utils.load_system_prompt import load_system_prompt
from src.states.MainState import MainState as State
from src.modules.Link import Link
from src.modules.Thought import Thought

tools = []


system_prompt = load_system_prompt("manager_agent")


class ManagerOutput(BaseModel):
    """manager agent output schema"""
    message: str = Field(
        description="The message generated by the manager agent for the next step. This message will be sent to the next agent.",
    )
    final_response: str = Field(
        description="The final response generated by the manager agent after finishing the task. Empty if the task is not finished.",
    )

    chain_of_thoughts: list[Thought] = Field(
        description="The thoughts generated by the agent at the current step.",
    )
    next_node: Literal['product_hunter_agent', 'researcher_agent', 'evaluator_agent', 'ask_human', 'end'] = Field(
        description="the next agent to call. Either 'product_hunter_agent' or 'researcher_agent' or 'evaluator_agent' or 'ask_human' or 'end'. If 'end', the agent has finished the task. If 'human', the agent is asking the user for more details.",
    )


# Define the function that calls the model
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.5,
).with_structured_output(ManagerOutput)


def manager_agent(state: State):
    if state.get("isHuman", False) == True:
        response = llm.invoke([
            SystemMessage(
                content=system_prompt
            ),
            HumanMessage(
                content=state["messages"][-1].content if state["messages"] else ""
            ),
        ])
        return {
            "messages": [AIMessage(response.message)],
            "chain_of_thoughts": [response.chain_of_thoughts],
            "next_node": response.next_node,
            "final_response": "",
        }

    # If the agent is not human, it means it is being called by another agent

    response = llm.invoke([
        SystemMessage(
            content=system_prompt
        ),
        HumanMessage(
            content=(f"{state['next_node']}: {state['messages'][-1].content}") if state["messages"] else ""),
    ])
    # If the model asks for more details, return its message to the human
    if response.next_node == "ask_human":
        return {
            "messages": [AIMessage(response.message)],
            "chain_of_thoughts": [response.chain_of_thoughts],
            "next_node": "ask_human",
            "final_response": "",
        }

    if response.final_response:
        # If the agent has finished the task, return the final response
        return {
            "messages": [AIMessage(response.final_response)],
            "chain_of_thoughts": [response.chain_of_thoughts],
            "next_node": "end",
            "final_response": response.final_response,
        }

    return {
        "messages": [AIMessage(response.message)],
        "chain_of_thoughts": [response.chain_of_thoughts],
        "next_node": response.next_node,
        "final_response": ""
    }
