from ast import operator
from typing import Annotated, Literal, TypedDict
from langchain.agents import AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferMemory
from langchain.agents import create_tool_calling_agent
from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import BaseModel, Field
import yaml
import uuid
from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from src.modules.llm import model
from src.utils.load_system_prompt import load_system_prompt
from src.states.MainState import MainState as State
from langchain_core.language_models.chat_models import BaseChatModel
from langgraph.types import Command
from time import sleep, time
from langgraph.graph import StateGraph, MessagesState, START, END
from src.modules.llm import model

# from src.modules.Thought import Thought

tools = []


system_prompt = load_system_prompt("manager_agent")


def make_supervisor_node(llm: BaseChatModel, members: list[str]) -> str:

    options = ["end"] + members
    system_prompt = load_system_prompt("manager_agent")

    class Router(BaseModel):
        """Worker to route to next. If no workers needed, route to FINISH."""
        message: str = Field(
            description="The message that will be sent to the next node (agent)"
        )
        next: Literal[*options]
        thought: str = Field(
            description="The thought generated by the agent at the current step. This will be used to determine the next step."
        )
        final_response: str = Field(
            description="When all information is gathered, construct the final response for the user here. IT MUST CONTAIN ALL WHAT THE USER REQUESTED. THIS IS THE OUTPUT THAT THE USER WILL SEE. This field will be empty if the agent is not finished.")

    def manager_node(state: State) -> Command[Literal[*members, "__end__"]]:
        """An LLM-based router."""
        sleep(1)
        messages = [
            {"role": "system", "content": system_prompt},
        ] + state["messages"]
        response = llm.with_structured_output(Router).invoke(messages)
        print(response)
        goto = response.next
        if goto == "end":
            goto = END

        return Command(goto=goto, update={"next": goto, "messages": [AIMessage(response.thought), HumanMessage(response.message, name="manager")]})

    return manager_node


manager_node = make_supervisor_node(
    model, ["researcher_agent", "product_hunter_agent"])
