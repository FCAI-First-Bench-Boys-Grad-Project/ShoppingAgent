from typing import Dict, List, Literal, Optional, Tuple
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage
from langchain_core.messages import HumanMessage, AIMessage
from src.modules.llm import model
from src.utils.load_system_prompt import load_system_prompt
from src.states.MainState import MainState as State
from langchain_core.language_models.chat_models import BaseChatModel
from langgraph.types import Command
from time import sleep
from langgraph.graph import END
from src.modules.llm import model


tools = []


system_prompt = load_system_prompt("manager_agent")


def make_supervisor_node(llm: BaseChatModel, members: list[str]) -> str:

    options = ["end"] + members
    system_prompt = load_system_prompt("manager_agent")

    class Router(BaseModel):
        """Worker to route to next. If no workers needed, route to FINISH."""
        message: str = Field(
            description="The message that will be sent to the next node (agent)"
        )
        next: Literal[*options]
        thought: str = Field(
            description="The thought generated by the agent at the current step. This will be used to determine the next step."
        )
        final_response: str = Field(
            description="When all information is gathered, construct the final response for the user here. IT MUST CONTAIN ALL WHAT THE USER REQUESTED. THIS IS THE OUTPUT THAT THE USER WILL SEE. This field will be empty if the agent is not finished.")
        products: List[str] = Field(
            description="When the final response is constructed, all product links provided to the user should be stored as a flat list alternating between product name and product link, e.g., [name1, url1, name2, url2, ...]. THIS MUST NOT BE EMPTY IF THERE ARE LINKS IN THE FINAL RESPONSE AND IT MUST CONTAIN THEM. THIS FIELD WILL BE AN EMPTY LIST IF THE AGENT IS NOT FINISHED."
        )

    def manager_node(state: State) -> Command[Literal[*members, "__end__"]]:
        """An LLM-based router."""
        sleep(1)
        messages = [
            {"role": "system", "content": system_prompt},
        ] + state["messages"]
        response = llm.with_structured_output(Router).invoke(messages)
        print(response)
        goto = response.next
        if goto == "end":
            goto = END

        return Command(goto=goto,
                       update={"next": goto,
                               "messages": [AIMessage(response.thought), HumanMessage(response.message, name="manager")],
                               "final_response": response.final_response,
                               "products": response.products,
                               "thoughts": [response.thought]
                               }
                       )

    return manager_node


manager_node = make_supervisor_node(
    model, ["researcher_agent", "product_hunter_agent"])
