from typing import Dict, List, Literal, Optional, Tuple
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage
from langchain_core.messages import HumanMessage, AIMessage
from src.modules.llm import model
from src.utils.load_system_prompt import load_system_prompt
from src.states.MainState import MainState as State
from langchain_core.language_models.chat_models import BaseChatModel
from langgraph.types import Command
from time import sleep
from langgraph.graph import END
from src.modules.llm import model


tools = []


system_prompt = load_system_prompt("manager_agent")


def make_supervisor_node(llm: BaseChatModel, members: list[str]) -> str:

    options = [END] + members
    system_prompt = load_system_prompt("manager_agent")

    class Router(BaseModel):
        """Worker to route to next. If no workers needed, route to FINISH."""
        message: str = Field(
            description="The message that will be sent to the next node (agent). Use this only for messages that will be directed to the next agent."
        )
        next: Literal[*options]
        thought: str = Field(
            description="The thought generated by the agent at the current step. This will be used to determine the next step."
        )
        final_response: str = Field(
            description="A message that will be sent to the user. It should not contain any kind of thinking process or messages to other agents.")
        products: List[str] = Field(
            description="When the final response is constructed, all product links provided to the user should be stored as a flat list alternating between product name and product link, e.g., [name1, url1, name2, url2, ...]. THIS MUST NOT BE EMPTY IF THERE ARE LINKS IN THE FINAL RESPONSE AND IT MUST CONTAIN THEM. THIS FIELD WILL BE AN EMPTY LIST IF THE AGENT IS NOT FINISHED."
        )

    def manager_node(state: State) -> Command[Literal[*members, "__end__"]]:
        """An LLM-based router."""
        sleep(1)
        messages = [
            {"role": "system", "content": system_prompt},
        ] + state["messages"]
        response = llm.with_structured_output(Router).invoke(messages)
        print(response)
        goto = response.next

        return Command(goto=goto,
                       update={"next": goto,
                               "messages": [AIMessage(response.thought), HumanMessage(response.message, name="manager")],
                               "final_response": response.final_response,
                               "products": response.products,
                               "thoughts": [response.thought]
                               }
                       )

    return manager_node


manager_node = make_supervisor_node(
    model, ["researcher_agent", "product_hunter_agent"])
